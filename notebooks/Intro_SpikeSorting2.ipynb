{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will walk through the basics of spike detection and sorting using the raw data files recorded by the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spike detection and sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two options. \n",
    "\n",
    "1) We can sort on our local computers, using WaveClus, a spikesorter that uses MATLAB. This is ok to do if you only have a couple of patients to sort. \n",
    "\n",
    "2) We can sort on the cluster. Preferred if you need to sort many subjects in parallel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Sort Locally on WaveClus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download: https://github.com/csn-le/wave_clus/tree/2565067a2c16bee2bbbcbb25d81cf634d4ca04a1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Sort on the cluster using Combinato (or Python sorter of choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, make sure you login to your Rhino account using the -XY flag to enable X11. You'll need this for a GUI later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, it is possibly better not to install combinato dependencies in an environment. Instead, run it in the base environment? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup environment for use with Combinato spike sorter\n",
    "\n",
    "1. ```conda create -n spike_clustering python=3.8 pyqt=5.9 numpy scipy xarray swig traits h5py matplotlib ipykernel qtpy```\n",
    "\n",
    "2. ```source activate spike_clustering```\n",
    "\n",
    "3. ```git clone https://github.com/jniediek/combinato.git```\n",
    "\n",
    "4. ```pip install tables```\n",
    "\n",
    "5. ```pip install PyWavelets```\n",
    "\n",
    "6. ```cd ~/combinato```\n",
    "\n",
    "7. ```python setup_options.py```\n",
    "\n",
    "8. ```PATH=$PATH:/HOME/combinato```\n",
    "\n",
    "9. ```PYTHONPATH=$PYTHONPATH:/HOME/```\n",
    "\n",
    "10. ```LD_LIBRARY_PATH=$HOME/miniconda3/lib:$LD_LIBRARY_PATH```\n",
    "\n",
    "11. ```export PATH PYTHONPATH LD_LIBRARY_PATH```\n",
    "\n",
    "12. ```python tools/test_installation.py```\n",
    "\n",
    "13. in /combinato/combinato/options.py, Line 46: ```folder_patterns``` should include ```’*chan*’```. This ensures that css-gui (you'll see this later) can actually find the results of the automatic spike sorting. \n",
    "\n",
    "14. in /combinato/combinato/extract/extract.py, Line 27: change \n",
    "```nWorkers = 5``` to ```nWorkers = 40```\n",
    "\n",
    "15. in /combinato/combinato/extract/extract.py, Line 83: change \n",
    "```starts = list(range(0, size, 32000*5*60))``` to ```starts = list(range(0, size, 30000*1*10))```\n",
    "\n",
    "16. in /combinato/combinato/extract/mp_extract.py, Line 113: change \n",
    "```sr = 32000.``` to ```sr = 30000.```\n",
    "\n",
    "# Add it to the kernels for jupyter\n",
    "\n",
    "1.  ```ipython kernel install --user --name=spike_clustering```\n",
    "\n",
    "# Select this kernel for your notebook\n",
    "\n",
    "# Download scripts to interface with Blackrock Recordings: \n",
    "https://www.blackrockmicro.com/wp-content/software/brPY.zip\n",
    "\n",
    "# (optional) Download spikeinterface for alternatives (i.e. Klusta): \n",
    "1. ```pip install spikeinterface```\n",
    "(see: https://spikeinterface.readthedocs.io/en/latest/installation.html)\n",
    "\n",
    "2. ```pip install Cython tqdm click klusta klustakwik2```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where is the data? \n",
    "\n",
    "The data you are looking for is raw microwire data recorded at 30 KHz in .ns5 or .ns6 files. For example:\n",
    "\n",
    "```/data10/RAM/subjects/R1278E/npt/20170224-145604_train_1/20170224-145604-001.ns6```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ******** IMPORTANT NOTE ******** \n",
    "\n",
    "## Do NOT, under any circumstance, run your spike sorting in the \"/data10/...\" folders! It will overwrite the current spike-sorting results. \n",
    "\n",
    "## Copy the data to your own directory or to your local computer. If you are CONFIDENT that your spike-sorting is an improvement on the results in \"/data10/...\" then feel free to replace it... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path as ospath\n",
    "import json\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the path for brPY library to jupyter path (replace with your own): \n",
    "import sys\n",
    "brPY_path = '/home1/salman.qasim/Salman_Python_Scripts'\n",
    "sys.path.append('{}/brPY'.format(brPY_path))\n",
    "from brpylib import NsxFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We record most of our single-neuron data using the BlackRock Microsystems Neuroport. This raw data is sampled at 30 kHz, meaning filesizes are very large, and is saved in proprietary .ns6 file formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the directory is where I copied the data to. You should replace with your own. \n",
    "test_data = '/home1/salman.qasim/Estes_Bootcamp/Data/20170224-145604-001.ns6'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These data can't be read natively by most spike sorters. It's full of insane headers that have to be stripped first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20170224-145604-001.ns6 opened\n"
     ]
    }
   ],
   "source": [
    "nsxfile = NsxFile(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output data requested is larger than 1 GB, attempting to preallocate output now\n",
      "\n",
      "First data packet requested begins at t = 0.003400 s, initial section padded with zeros\n",
      "Data extraction requires paging: 1 of 4\n",
      "Data extraction requires paging: 2 of 4\n",
      "Data extraction requires paging: 3 of 4\n",
      "Data extraction requires paging: 4 of 4\n"
     ]
    }
   ],
   "source": [
    "cont_data = nsxfile.getdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20170224-145604-001.ns6 closed\n"
     ]
    }
   ],
   "source": [
    "nsxfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['elec_ids', 'start_time_s', 'data_time_s', 'downsample', 'data', 'data_headers', 'ExtendedHeaderIndices', 'samp_per_s'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_data.keys()\n",
    "# Let's split a lot of this useful information into a params file and grab the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the ephys \n",
    "data = cont_data.pop('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split electrode 0\n",
      "split electrode 1\n",
      "split electrode 2\n",
      "split electrode 3\n",
      "split electrode 4\n",
      "split electrode 5\n",
      "split electrode 6\n",
      "split electrode 7\n",
      "split electrode 8\n",
      "split electrode 9\n",
      "split electrode 10\n",
      "split electrode 11\n",
      "split electrode 12\n",
      "split electrode 13\n",
      "split electrode 14\n",
      "split electrode 15\n",
      "split electrode 16\n",
      "split electrode 17\n",
      "split electrode 18\n",
      "split electrode 19\n",
      "split electrode 20\n",
      "split electrode 21\n",
      "split electrode 22\n",
      "split electrode 23\n",
      "split electrode 24\n",
      "split electrode 25\n",
      "split electrode 26\n",
      "split electrode 27\n",
      "split electrode 28\n",
      "split electrode 29\n",
      "split electrode 30\n",
      "split electrode 31\n"
     ]
    }
   ],
   "source": [
    "# For Combinato (separate channels - save as matfiles)\n",
    "## NOTE: CURRENTLY, Combinato hangs on large datasets. \n",
    "\n",
    "# Open a text file and write in header information \n",
    "path = ospath.splitext(nsxfile.datafile.name)[0]\n",
    "with open('{}_params.txt'.format(path), 'w') as file:\n",
    "     file.write(json.dumps(cont_data))\n",
    "    \n",
    "# Split the data by channels and write into hdf5 files\n",
    "for ix, electrode in enumerate(cont_data['elec_ids']):\n",
    "    if electrode != '129': # This is typically the sync pulse channel \n",
    "        ## HDF5: \n",
    "        f = h5py.File(\"{}_chan{}.hdf5\".format(path, electrode), \"w\")\n",
    "        f.create_dataset('data', data=data[ix, :], dtype='<i2')\n",
    "        f.close()\n",
    "        print('split electrode {}'.format(ix))\n",
    "        #  Using the blackrock code\n",
    "        #nsxfile.savesubsetnsx(elec_ids=[electrode], file_suffix='chan_{}'.format(electrodes))\n",
    "        # Matfile: \n",
    "        #data_as_dict = {'data': data[ix, :11000000].astype('double'),\n",
    "        #                 'sr':30000}\n",
    "        #sio.savemat(\"{}_chan{}.mat\".format(path, electrode), data_as_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For Klusta (all channels together, no splitting - HDF5)\n",
    "\n",
    "# # Grab the ephys \n",
    "# data = cont_data.pop('data')\n",
    "# # Open a text file and write in header information \n",
    "# path = ospath.splitext(nsxfile.datafile.name)[0]\n",
    "# with open('{}_params.txt'.format(path), 'w') as file:\n",
    "#      file.write(json.dumps(cont_data))\n",
    "    \n",
    "# # save the data to hdf5 \n",
    "# f = h5py.File(\"{}.hdf5\".format(path), \"w\")\n",
    "# f.create_dataset('dataset', data=data, dtype='<i2')\n",
    "# f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a bash script to run the automatic spike sorting for all the data \n",
    "# Note, because Combinato currently doesn't support parallel processing for non-Neuralynx data, we use css-simple-clustering. \n",
    "\n",
    "path_to_sh = '/home1/salman.qasim/Estes_Bootcamp'\n",
    "path_to_split_files= '/home1/salman.qasim/Estes_Bootcamp/Data'\n",
    "path_to_save_jobfiles= '/home1/salman.qasim/Estes_Bootcamp/Combinato_clustering'\n",
    "user_name = path_to_sh.split('/')[2][0:3]\n",
    "\n",
    "\n",
    "# Extract and cluster NEGATIVE polarity spikes \n",
    "with open ('{}/run_combinato_neg.sh'.format(path_to_sh), 'w') as bash_script:\n",
    "        bash_script.write('''\n",
    "                    #!/bin/bash\n",
    "                    declare -a arr=({path_to_split_files})\n",
    "                    mkdir {path_to_save_jobfiles}\n",
    "                    mkdir {path_to_save_jobfiles}/$(date '+%d-%b-%Y')\n",
    "                    cd {path_to_save_jobfiles}/$(date '+%d-%b-%Y')\n",
    "                    touch do_manual_neg.txt\n",
    "                    for file in \"${{arr[@]}}\"; do\n",
    "                        cd $file\n",
    "                        for chan in *.mat; do\n",
    "                            css-extract --files ${{chan%%.*}}.hdf5 --h5\n",
    "                        css-mask-artifacts\n",
    "                        ls */*.h5 > do_sort.txt\n",
    "                        css-prepare-sorting --neg --jobs do_sort.txt\n",
    "                        css-cluster --jobs do_sort.txt\n",
    "                        css-combine --jobs do_sort.txt\n",
    "                        done\n",
    "                    done\n",
    "                '''.format(path_to_split_files=path_to_split_files,\n",
    "                           path_to_save_jobfiles=path_to_save_jobfiles,\n",
    "                           user_name=user_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's probably a way to run this bash script well from your notebook. But I couldn't figure that out (see commented code below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# subprocess.call(['{}/run_combinato.sh'.format(path_to_sh)], shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, head to your terminal. \n",
    "\n",
    "Make sure you type in\n",
    "\n",
    "``` source activate spike_clustering```\n",
    "\n",
    "to ensure you're in the proper environment. Then, \n",
    "\n",
    "```cd /home1/salman.qasim/Estes_Bootcamp/Data```\n",
    "\n",
    "``` bash run_combinato_neg.sh```\n",
    "\n",
    "This might take some time. This is the automatic clustering, without human input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster POSITIVE polarity spikes \n",
    "with open ('{}/run_combinato_pos.sh'.format(path_to_sh), 'w') as bash_script:\n",
    "        bash_script.write('''\n",
    "                    #!/bin/bash\n",
    "                    declare -a arr=({path_to_split_files})\n",
    "                    mkdir {path_to_save_jobfiles}\n",
    "                    mkdir {path_to_save_jobfiles}/$(date '+%d-%b-%Y')\n",
    "                    cd {path_to_save_jobfiles}/$(date '+%d-%b-%Y')\n",
    "                    touch do_manual_neg.txt\n",
    "                    for file in \"${{arr[@]}}\"; do\n",
    "                        cd $file\n",
    "                        ls */*.h5 > do_sort.txt\n",
    "                        css-prepare-sorting --jobs do_sort.txt\n",
    "                        css-cluster --jobs do_sort.txt\n",
    "                        css-combine --jobs do_sort.txt\n",
    "                        done\n",
    "                    done\n",
    "                '''.format(path_to_split_files=path_to_split_files,\n",
    "                           path_to_save_jobfiles=path_to_save_jobfiles,\n",
    "                           user_name=user_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, head to your terminal. \n",
    "\n",
    "Make sure you type in\n",
    "\n",
    "``` source activate spike_clustering```\n",
    "\n",
    "to ensure you're in the proper environment. Then, \n",
    "\n",
    "```cd /home1/salman.qasim/Estes_Bootcamp/Data```\n",
    "\n",
    "``` bash run_combinato_pos.sh```\n",
    "\n",
    "This might take some time. This is the automatic clustering, without human input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is done, there should be a lot of new folders in \"/home1/salman.qasim/Estes_Bootcamp/Data\".\n",
    "\n",
    "Before you can use the GUI to manually sort through these results, you HAVE TO DEACTIVATE your environment. This is dumb! But otherwise Python can't find the plugin for displaying the GUI, which is not environment specific. \n",
    "\n",
    "``` source deactivate spike_clustering```\n",
    "\n",
    "```cd /home1/salman.qasim/Estes_Bootcamp/Data```\n",
    "\n",
    "```css-gui```\n",
    "\n",
    "This should bring up the GUI for manual sorting. If you did step 13 of the Combinato setup correctly, you should be able to see the different channel folders when you select \"Open\" from the drop-down menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Everything below here is SpikeInterface test code that I haven't quite figured out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Spikeinterfaces example using klusta\n",
    "# import spikeinterface.sorters as ss\n",
    "# import spikeinterface.toolkit as st\n",
    "# f = h5py.File('/home1/salman.qasim/Estes_Bootcamp/Data/20170224-145604-001_chan101.hdf5', 'r')\n",
    "# timeseries =  f['data'].value\n",
    "# timeseries = timeseries.reshape([1, len(timeseries)])\n",
    "# sampling_frequency = 30000\n",
    "# recording = se.NumpyRecordingExtractor(timeseries=timeseries, geom=None, sampling_frequency=sampling_frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting_KL = ss.run_klusta(recording, output_folder='/home1/salman.qasim/Estes_Bootcamp/Data/tmp_KL', verbose=True)\n",
    "# print('Units found with Klusta:', sorting_KL.get_unit_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# templates = st.postprocessing.get_unit_templates(recording, sorting_KL, max_spikes_per_unit=200,\n",
    "#                                                  save_as_property=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# firing_rates = st.validation.compute_firing_rates(sorting_KL, duration_in_frames=recording.get_num_frames())\n",
    "# isi_violations = st.validation.compute_isi_violations(sorting_KL, duration_in_frames=recording.get_num_frames(), isi_threshold=0.0015)\n",
    "# snrs = st.validation.compute_snrs(recording=recording, sorting=sorting_KL, max_spikes_per_unit_for_snr=1000)\n",
    "# nn_hit_rate, nn_miss_rate = st.validation.compute_nn_metrics(recording=recording, sorting=sorting_KL, num_channels_to_compare=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# st.postprocessing.export_to_phy(recording, sorting_KL, output_folder='phy', verbose=True)\n",
    "# # Or from a notebook:  !phy template-gui phy/params.py\n",
    "# # After manual curation you can load back the curated data using the PhySortingExtractor:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
